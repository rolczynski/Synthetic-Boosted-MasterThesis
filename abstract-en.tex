
\begin{abstract}

\noindent Nowadays, speech recognition is an active research field, where various
deep neural architectures are explored.
The published successful models are optimized on massive transcribed datasets,
which most of them are closed.
A deep neural network solves two closely related tasks.
It learns to recognize phonemes and to formulate grammar rules at the same time.
In fact, a model is able to parallel and accurate build both of them, when a training
corpus is large enough.
However, inflected languages such as~Polish contains much more
grammar rules to define than in the case of English.
Therefore, in order to achieve comparable results in the Polish language, the corpus must
be substantially larger than the one presented for the English language.
In contrast, to build more massive datasets, we present the \textit{Synthetic~Boosted~Model},
which is an attempt to use synthetic data to enrich more profound the implicit language model.
In the presented work, we propose the new model architecture, the new objective function, and the new training policy.

\vspace{1.5cm}
\noindent\textbf{Keywords}: automatic speech recognition, deep neural network,
recurrent neural network, connectionist temporal classification, data augmentation, synthetic data, synthetic boosted

\end{abstract}
